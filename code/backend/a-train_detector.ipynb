{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edbfc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "======================================================================\n",
    "SCRIPT DE ENTRENAMIENTO: DETECTOR DE ANOMALIAS (PyTorch VGG16)\n",
    "======================================================================\n",
    "Versión Modular para Notebooks.\n",
    "Estructura:\n",
    "1. Configuración\n",
    "2. Dataset y Dataloaders\n",
    "3. Modelo VGG16 Autoencoder\n",
    "4. Funciones de Entrenamiento\n",
    "5. Funciones de Evaluación (Cálculo de Umbral con Filtro Outliers)\n",
    "6. Orquestador Principal\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import traceback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Configurar Matplotlib para evitar errores de GUI en servidores\n",
    "plt.switch_backend('Agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad404084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOQUE 1: CONFIGURACIÓN ---\n",
    "\n",
    "try:\n",
    "    from dataset_paths import (\n",
    "        DATASET_BASE_PATH,\n",
    "        DatasetPaths,\n",
    "        AVAILABLE_CATEGORIES,\n",
    "        discover_categories,\n",
    "        DETECTOR_MODEL_PATH\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: No se pudo importar 'dataset_paths.py'.\")\n",
    "    raise\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "SENSE = 1.0 \n",
    "\n",
    "def get_device():\n",
    "    return DEVICE\n",
    "\n",
    "def get_next_model_version_path_torch(base_dir, prefix, category):\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "    max_num = 0\n",
    "    base_prefix = f\"{prefix}_{category}\"\n",
    "    for f in os.listdir(base_dir):\n",
    "        if f.startswith(base_prefix + \"_\") and f.endswith(\".pth\"):\n",
    "            try:\n",
    "                num = int(f[len(base_prefix)+1:-4])\n",
    "                if num > max_num: max_num = num\n",
    "            except ValueError: continue\n",
    "    return os.path.join(base_dir, f\"{base_prefix}_{str(max_num + 1).zfill(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "222be761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOQUE 2: DATASET Y DATALOADERS ---\n",
    "\n",
    "class DefectDataset(Dataset):\n",
    "    def __init__(self, folder_path, transform=None):\n",
    "        self.folder_path = folder_path\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        if os.path.exists(folder_path):\n",
    "            self.image_files = [f for f in os.listdir(folder_path) \n",
    "                                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.folder_path, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Autoencoder: input == target\n",
    "        return image, image\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def prepare_dataloaders(category, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Prepara y devuelve (train_loader, anomaly_loader).\n",
    "    Si no hay anomalías para test, anomaly_loader será None.\n",
    "    \"\"\"\n",
    "    print(f\"--- Preparando datos para: {category} ---\")\n",
    "    paths = DatasetPaths(DATASET_BASE_PATH, category)\n",
    "    \n",
    "    # 1. Train (Buenas)\n",
    "    train_dataset = DefectDataset(paths.train_path, transform=data_transforms)\n",
    "    if len(train_dataset) == 0:\n",
    "        print(f\"Error: No hay imágenes en {paths.train_path}\")\n",
    "        return None, None\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 2. Anomalías (Test)\n",
    "    defect_folders = paths.get_test_defect_folders()\n",
    "    anomaly_files = []\n",
    "    for df in defect_folders:\n",
    "        ds = DefectDataset(os.path.join(paths.test_path, df), transform=data_transforms)\n",
    "        if len(ds) > 0: anomaly_files.append(ds)\n",
    "    \n",
    "    if anomaly_files:\n",
    "        anomaly_dataset = torch.utils.data.ConcatDataset(anomaly_files)\n",
    "        anomaly_loader = DataLoader(anomaly_dataset, batch_size=batch_size, shuffle=False)\n",
    "    else:\n",
    "        anomaly_loader = None\n",
    "    \n",
    "    print(f\"  Train Images: {len(train_dataset)}\")\n",
    "    print(f\"  Anomaly Images: {len(anomaly_dataset) if anomaly_files else 0}\")\n",
    "    return train_loader, anomaly_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a4d9424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOQUE 3: MODELO ---\n",
    "\n",
    "class VGG16Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Autoencoder, self).__init__()\n",
    "        # Encoder (Congelado)\n",
    "        vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        self.encoder = nn.Sequential(*list(vgg.features.children())[:24])\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Decoder (Entrenable)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        return self.decoder(features)\n",
    "\n",
    "def get_model(weights_path=None):\n",
    "    \"\"\"Instancia el modelo y opcionalmente carga pesos.\"\"\"\n",
    "    model = VGG16Autoencoder().to(DEVICE)\n",
    "    if weights_path:\n",
    "        if os.path.exists(weights_path):\n",
    "            print(f\"Cargando pesos desde: {weights_path}\")\n",
    "            model.load_state_dict(torch.load(weights_path, map_location=DEVICE))\n",
    "        else:\n",
    "            print(f\"Advertencia: No se encontró {weights_path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5151e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOQUE 4: ENTRENAMIENTO ---\n",
    "\n",
    "def train_model(model, train_loader, epochs=EPOCHS):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "    model.train()\n",
    "    \n",
    "    print(f\"--- Iniciando Entrenamiento ({epochs} épocas) ---\")\n",
    "    start_time = time.time()\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for i, (inputs, _) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Barra de Progreso\n",
    "            percent = (i + 1) / total_batches\n",
    "            bar = '=' * int(20 * percent) + '-' * (20 - int(20 * percent))\n",
    "            sys.stdout.write(f\"\\r    Epoch {epoch+1:03d}/{epochs} [{bar}] Loss: {loss.item():.6f}\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        duration = time.time() - epoch_start\n",
    "        sys.stdout.write(f\"\\r    Epoch {epoch+1:03d}/{epochs} [{'='*20}] Avg Loss: {epoch_loss:.6f} | Time: {duration:.1f}s   \\n\")\n",
    "\n",
    "    print(f\"  Entrenamiento completado en {(time.time() - start_time)/60:.1f} min.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c07ef507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOQUE 5: EVALUACIÓN Y UMBRAL (Con Filtro de Outliers) ---\n",
    "\n",
    "def calculate_errors(model, dataloader):\n",
    "    \"\"\"Calcula MSE por imagen.\"\"\"\n",
    "    model.eval()\n",
    "    mse = nn.MSELoss(reduction='none')\n",
    "    errors = []\n",
    "    print(\"  Calculando errores...\", end=\"\", flush=True)\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in dataloader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            loss = mse(inputs, outputs).mean(dim=[1, 2, 3])\n",
    "            errors.extend(loss.cpu().numpy())\n",
    "    print(\" Listo.\")\n",
    "    return np.array(errors)\n",
    "\n",
    "def remove_outliers(data):\n",
    "    \"\"\"\n",
    "    Filtra outliers usando el Rango Intercuartílico (IQR).\n",
    "    Retorna los datos limpios (sin valores extremos superiores).\n",
    "    \"\"\"\n",
    "    if len(data) == 0: return data\n",
    "    \n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Límite superior para considerar un error como \"normal\"\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    clean_data = data[data <= upper_bound]\n",
    "    \n",
    "    removed = len(data) - len(clean_data)\n",
    "    if removed > 0:\n",
    "        print(f\"  [FILTRO] Se eliminaron {removed} outliers (> {upper_bound:.6f})\")\n",
    "        \n",
    "    return clean_data\n",
    "\n",
    "def compute_threshold(good_errors, sensitivity=SENSE):\n",
    "    \"\"\"Calcula el umbral usando estadísticas de datos limpios.\"\"\"\n",
    "    # 1. Limpiar\n",
    "    clean_errors = remove_outliers(good_errors)\n",
    "    \n",
    "    if len(clean_errors) == 0:\n",
    "        print(\"  [ADVERTENCIA] Se filtraron todos los datos. Usando originales.\")\n",
    "        clean_errors = good_errors\n",
    "\n",
    "    # 2. Estadísticas\n",
    "    mean = np.mean(clean_errors)\n",
    "    std = np.std(clean_errors)\n",
    "    \n",
    "    # 3. Umbral\n",
    "    threshold = mean + sensitivity * std\n",
    "    \n",
    "    # 4. Validación de seguridad\n",
    "    max_clean = np.max(clean_errors)\n",
    "    if threshold < max_clean:\n",
    "        print(f\"  [AJUSTE] Umbral ({threshold:.6f}) < Max Limpio ({max_clean:.6f}). Ajustando.\")\n",
    "        threshold = max_clean * 1.05\n",
    "        \n",
    "    return threshold, mean, std\n",
    "\n",
    "def save_results(model, category, threshold, mean, std):\n",
    "    base_path = get_next_model_version_path_torch(DETECTOR_MODEL_PATH, \"detector\", category)\n",
    "    torch.save(model.state_dict(), base_path + \".pth\")\n",
    "    \n",
    "    meta = {\n",
    "        \"category\": category,\n",
    "        \"threshold\": float(threshold),\n",
    "        \"stats\": {\"mean\": float(mean), \"std\": float(std)}\n",
    "    }\n",
    "    with open(base_path + \"_threshold.json\", \"w\") as f:\n",
    "        json.dump(meta, f, indent=4)\n",
    "    print(f\"  Modelo guardado en: {base_path}.pth\")\n",
    "\n",
    "def plot_results(good_errors, anomaly_errors, threshold, category):\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Histograma datos originales (gris)\n",
    "        plt.hist(good_errors, bins=50, alpha=0.3, label='Buenas (Original)', color='gray', density=True)\n",
    "        \n",
    "        # Histograma datos limpios (azul)\n",
    "        clean_errors = remove_outliers(good_errors)\n",
    "        plt.hist(clean_errors, bins=50, alpha=0.7, label='Buenas (Limpias)', color='blue', density=True)\n",
    "\n",
    "        if len(anomaly_errors) > 0:\n",
    "            plt.hist(anomaly_errors, bins=50, alpha=0.7, label='Anomalias', color='red', density=True)\n",
    "            \n",
    "        plt.axvline(threshold, color='black', linestyle='dashed', label=f'Umbral: {threshold:.5f}')\n",
    "        plt.title(f\"Distribución de Errores - {category}\")\n",
    "        plt.legend()\n",
    "        \n",
    "        plot_path = os.path.join(DETECTOR_MODEL_PATH, f\"distribucion_errores_{category}.png\")\n",
    "        # Asegurar que el directorio existe antes de guardar\n",
    "        os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "        print(f\"  Gráfico guardado en: {plot_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error al graficar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb16bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BLOQUE 6: ORQUESTADOR ---\n",
    "\n",
    "def run_full_pipeline():\n",
    "    print(f\"Dispositivo: {DEVICE}\")\n",
    "    categories = discover_categories(DATASET_BASE_PATH)\n",
    "    \n",
    "    for category in categories:\n",
    "        try:\n",
    "            # 1. Preparar\n",
    "            train_dl, anomaly_dl = prepare_dataloaders(category)\n",
    "            if not train_dl: continue\n",
    "            \n",
    "            # 2. Entrenar\n",
    "            model = get_model()\n",
    "            model = train_model(model, train_dl, epochs=EPOCHS)\n",
    "            \n",
    "            # 3. Evaluar (Usamos train_dl sin shuffle para consistencia)\n",
    "            eval_dl = DataLoader(train_dl.dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            good_errors = calculate_errors(model, eval_dl)\n",
    "            \n",
    "            anomaly_errors = np.array([])\n",
    "            if anomaly_dl:\n",
    "                anomaly_errors = calculate_errors(model, anomaly_dl)\n",
    "            \n",
    "            # 4. Calcular Umbral con Outliers Filtrados\n",
    "            threshold, mean, std = compute_threshold(good_errors, SENSE)\n",
    "            print(f\"  RESULTADO: Umbral={threshold:.6f} (SENSE={SENSE})\")\n",
    "            \n",
    "            # 5. Guardar\n",
    "            plot_results(good_errors, anomaly_errors, threshold, category)\n",
    "            save_results(model, category, threshold, mean, std)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en categoría {category}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_full_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
